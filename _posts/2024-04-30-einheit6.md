---
title: "Lerneinheit 6: Metadaten modellieren und Schnittstellen nutzen A (OpenRefine)"
date: 2024-04-30
---
Diese und vergangene Woche haben wir uns im Unterricht mit dem Thema Metadaten modellieren anhand der Software OpenRefine beschäftigt. Für mich war die Auseinandersetzung mit diesem Tool insbesondere im Hinblick auf die Datenbereinigung interessant. Parallel zu diesem Kurs besuche ich nämlich den Kurs "Datenanalyse", wo wir die Datenbereinigung "händisch" über einen Python Code vornehmen. Es ist eindrücklich zu sehen, wieviel Zeilen Code für eine Aktion nötig sind, die mit einem "Klick" in der graphischen Oberfläche von OpenRefine erledigt werden können (z.B. das Aufteilen der Zeilen bei mehreren Autor:innen, das Einlesen der Daten über eine URL). Besonders passend finde ich die "Diamant-Analogie", die durch das Logo von OpenRefine angedeutet wird: Die Rohdaten werden geschliffen und verfeinert, damit die Kostbarkeit der vorhandenen Daten zur Geltung kommt.

**Was habe ich heute gelernt? Was war für mich neu?**
OpenRefine ist ein leistungsstarkes ("mächtiges"), kostenloses Open-Source-Tool zur Bearbeitung unsauberer ("messy") Daten, wie sie in Bibliotheken und Archiven häufig vorkommen (siehe auch [Lerneinheit 3](https://patriciaschneiderfhgr.github.io/Lerntagebuch_BAIN/2024/02/27/einheit3.html), Hinweis zum Metadata Quality Assessment Framework von Péter Király. OpenRefine wird grösstenteils von Bibliotheken genutzt, allerdings hat sich die Nutzergruppenverteilung in den letzten Jahren etwas ausgeglichen (siehe Abbildung --> Librarians (15.1%), Cultural sector professionals (11.6%), Linked Open Data / semantic web aficionadas (11.2%), Researchers (10.1%) and Data scientists (9.5%)).

![2022-survey-communities-d245b55a72d94bb4005ca3ee29242889](https://github.com/user-attachments/assets/021e0165-0b37-44f0-95ca-0498d24fbd09)

*Quelle: https://openrefine.org/blog/2022/06/28/2022-survey-results.html*

Nach einer kurzen Einführung durch Herr Lohmeier, wird mir auch schnell bewusst, warum das Tool so verbreitet ist. Die Vorteile - neben den umfassenden Funktionalitäten zur Datenbereinigung, -anreicherung und -exportierung - liegen für mich v.a. in der aktiven Community über die Bibliothekswelt hinaus, in der Privatheit der Daten (die Daten werden lokal und nicht über eine Cloud bearbeitet) und in der (im Vergleich zur Datenanalyse mit "händischer Programmierlogik") intuitiveren Bedien- und Erlernbarkeit, die aber dennoch einiges an Hintergrundwissen (z.B. MARC21 Format, XML Format) und Programmierlogik-Kenntnissen erfordert (z.B. wie man Fehlermeldungen abfängt, Template Exporter). Einziger Negativpunkt: Die Bedienung auf einem kleineren Bildschirm (z.B. Laptop) ist schwierig - das Responsive-Design ist leider nicht vollständig umgesetzt (z.B. Pop-Ups müssen ständig verschoben werden). Das erschwert mir im Unterricht das Umsetzen der Übungen und lässt mich schlussendlich einfach der Demonstration über Webex folgen, ohne selber nochmals gross Hand anzulegen.

**Exkurs messy data**: Ich habe mich gefragt, warum denn überhaupt in Bibliotheken so oft unbereinigte Daten vorkommen? Im Austausch mit meinen Arbeitskolleg:innen haben wir ein paar Punkte für unsere Bibliothek zusammengetragen:
1. **Manuelle Dateneingabe**: Daten werden häufig von Menschen eingegeben, was zu Tippfehlern, Inkonsistenzen und anderen Fehlern führen kann.
2. **Unterschiedliche Datenquellen**: Wir erhalten Daten und Metadaten über Schnittstellen aus verschiedenen Quellen (Buchzentrum, SBD, Verlage etc.), die unterschiedliche Formate und Standards verwenden, was zu Inkonsistenzen in Begriffs- und Feldverwendung führt. Die Qualität der Daten variiert, da sie oft von unterschiedlichen Personen mit unterschiedlichen Kenntniss- und Sorgfaltsebenen erstellt werden.
3. **Historische Daten und technologische Veränderungen**: Alte Datenbestände wurden mit unterschiedlichen Systemen und Standards erfasst und nicht immer korrekt aktualisiert oder bereinigt - die Datenmenge ist schlicht zu gross dafür. Wechselnde Technologien und Systeme können zu Dateninkonsistenzen führen, besonders wenn Daten migriert oder konvertiert werden.
**Fazit**: Wir sehen die Notwendigkeit für eine Datenbereinigung durchaus - dazu fehlen aber, wie so oft, Zeit und Ressourcen. Die pragmatische Lösung war bisher, dass Datensätze nicht systematisch bereinigt, sondern nur, wenn man "zufällig" darauf stösst korrigiert werden. Im Hinblick auf die Umstellung auf ein neues LMS und den Export der Daten auf ein neues System, werden umfassende Analysen zum Daten-Export gemacht, wo schätzungsweise gewisse Datensatztypen soweit möglich systematisch und automatisiert bereinigt werden sollen. Welches Tool dafür genutzt wird, ist uns nicht bekannt, da dafür eine externe IT-Firma beauftragt wurde.
